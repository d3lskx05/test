# app.py
import streamlit as st
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
import re
import numpy as np
import io
import os
import json
import requests
from datetime import datetime

# -----------------------------
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
# -----------------------------
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
CHUNK_SIZE = 800
TOP_K = 3
CONTEXT_LIMIT = 2000  # —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è HF API
DATA_DIR = "data_storage"
os.makedirs(DATA_DIR, exist_ok=True)

# -----------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ embedder
# -----------------------------
@st.cache_resource(show_spinner=False)
def load_embedder():
    return SentenceTransformer(EMBED_MODEL)

embedder = load_embedder()

# -----------------------------
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# -----------------------------
def clean_text(text: str) -> str:
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_text_from_pdf_bytes(file_bytes) -> str:
    reader = PdfReader(io.BytesIO(file_bytes))
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return text

def chunk_text(text: str, max_chars: int = CHUNK_SIZE):
    words = text.split()
    chunks = []
    cur = []
    cur_len = 0
    for w in words:
        cur.append(w)
        cur_len += len(w) + 1
        if cur_len >= max_chars:
            chunks.append(" ".join(cur))
            cur = []
            cur_len = 0
    if cur:
        chunks.append(" ".join(cur))
    return chunks

def build_embeddings(chunks):
    return np.array(embedder.encode(chunks, show_progress_bar=False))

def save_embeddings(doc_name, chunks, embeddings):
    path = os.path.join(DATA_DIR, f"{doc_name}_chunks.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump({"chunks": chunks, "embeddings": embeddings.tolist()}, f)

def load_embeddings(doc_name):
    path = os.path.join(DATA_DIR, f"{doc_name}_chunks.json")
    if not os.path.exists(path):
        return None, None
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
        return data["chunks"], np.array(data["embeddings"])

def retrieve_top_k(question, all_chunks, all_embeddings, top_k=TOP_K):
    if not all_chunks or not all_embeddings:
        return []
    q_emb = embedder.encode([question], show_progress_bar=False)[0]
    best = []
    for chunks, emb in zip(all_chunks, all_embeddings):
        if emb.size == 0:
            continue
        norms = np.linalg.norm(emb, axis=1) * (np.linalg.norm(q_emb) + 1e-9)
        sims = (emb @ q_emb) / norms
        idxs = np.argsort(sims)[::-1][:top_k]
        for i in idxs:
            best.append((sims[i], chunks[i]))
    best_sorted = sorted(best, key=lambda x: x[0], reverse=True)[:top_k]
    return [chunk for (_, chunk) in best_sorted]

def highlight_terms(snippet: str, question: str):
    terms = set([w.lower() for w in re.findall(r"\w{3,}", question)])
    escaped = re.escape
    out = snippet
    for t in terms:
        out = re.sub(r'(?i)(' + escaped(t) + r')', r'<mark>\1</mark>', out)
    return out

def extract_money_percent_dates(text: str):
    items = {}
    perc = re.findall(r'(\d+(?:[.,]\d+)?\s?%|\d+(?:[.,]\d+)?\s?–ø—Ä–æ—Ü)', text, flags=re.IGNORECASE)
    items['percents'] = list(set(perc))
    sums = re.findall(r'(\d{1,3}(?:[ \u00A0]\d{3})*(?:[.,]\d+)?\s?(?:‚ÇΩ|—Ä—É–±|—Ä—É–±\.|RUB)?)', text)
    items['sums'] = list(set(sums))
    dates = re.findall(r'(\d{1,2}[./-]\d{1,2}[./-]\d{2,4})', text)
    items['dates'] = list(set(dates))
    return items

# -----------------------------
# Backend: HF API –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
# -----------------------------
HF_API_URL = "https://api-inference.huggingface.co/models/google/flan-t5-base"
HF_API_TOKEN = st.secrets["HF_API_TOKEN"]
CACHE_PATH = os.path.join(DATA_DIR, "hf_cache.json")
if os.path.exists(CACHE_PATH):
    with open(CACHE_PATH, "r", encoding="utf-8") as f:
        hf_cache = json.load(f)
else:
    hf_cache = {}

def generate_answer(question: str, context: str):
    key = f"{question}||{context[:CONTEXT_LIMIT]}"
    if key in hf_cache:
        return hf_cache[key]
    prompt = f"""–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ –∏ —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç–∞–º–∏.
–í–æ–ø—Ä–æ—Å: {question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context[:CONTEXT_LIMIT]}

–û—Ç–≤–µ—Ç:"""
    headers = {"Authorization": f"Bearer {HF_API_TOKEN}"}
    payload = {"inputs": prompt, "parameters": {"max_new_tokens": 200}}
    try:
        response = requests.post(HF_API_URL, headers=headers, json=payload, timeout=60)
        response.raise_for_status()
        output = response.json()
        if isinstance(output, list) and "generated_text" in output[0]:
            answer = output[0]["generated_text"].strip()
        else:
            answer = str(output)
    except Exception as e:
        answer = f"–û—à–∏–±–∫–∞ API: {e}"
    hf_cache[key] = answer
    with open(CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(hf_cache, f)
    return answer

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="–ë–∞–Ω–∫–æ–≤—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫ (MVP)", layout="wide")
st.title("üìÑ –ë–∞–Ω–∫–æ–≤—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫ ‚Äî –∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ (MVP)")

if "docs" not in st.session_state:
    st.session_state.docs = []
if "chunks" not in st.session_state:
    st.session_state.chunks = []
if "embeddings" not in st.session_state:
    st.session_state.embeddings = []
if "history" not in st.session_state:
    st.session_state.history = []

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
    uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å PDF", type=["pdf"], accept_multiple_files=True)
    if uploaded:
        for f in uploaded:
            raw = f.read()
            txt = clean_text(extract_text_from_pdf_bytes(raw))
            st.session_state.docs.append({"name": f.name, "text": txt})
        st.success(f"{len(uploaded)} —Ñ–∞–π–ª(–æ–≤) –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")

    if st.button("–û–±–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å"):
        st.session_state.chunks = []
        st.session_state.embeddings = []
        for doc in st.session_state.docs:
            ch, emb = load_embeddings(doc["name"])
            if ch and emb is not None:
                st.session_state.chunks.append(ch)
                st.session_state.embeddings.append(emb)
            else:
                ch = chunk_text(doc["text"])
                emb = build_embeddings(ch)
                save_embeddings(doc["name"], ch, emb)
                st.session_state.chunks.append(ch)
                st.session_state.embeddings.append(emb)
        st.success("–ò–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª—ë–Ω ‚úÖ")

    if st.button("–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ"):
        st.session_state.docs = []
        st.session_state.chunks = []
        st.session_state.embeddings = []
        st.session_state.history = []

# Main area
if not st.session_state.docs:
    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF –∏ –æ–±–Ω–æ–≤–∏—Ç–µ –∏–Ω–¥–µ–∫—Å.")
    st.stop()

# –ü–æ–∫–∞–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
col1, col2 = st.columns([3,1])
with col1:
    st.subheader("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
    for i, doc in enumerate(st.session_state.docs):
        with st.expander(f"{i+1}. {doc['name']}", expanded=False):
            snippet = doc['text'][:1000] + ("..." if len(doc['text'])>1000 else "")
            st.write(snippet)
            items = extract_money_percent_dates(doc['text'])
            st.markdown("**–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (–ø—Ä–æ—Ü–µ–Ω—Ç—ã/—Å—É–º–º—ã/–¥–∞—Ç—ã):**")
            st.write(items)
with col2:
    st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    doc_names = [d['name'] for d in st.session_state.docs]
    sel_compare = st.multiselect("–í—ã–±—Ä–∞—Ç—å 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞", options=doc_names, max_selections=2)
    if st.button("–°—Ä–∞–≤–Ω–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ"):
        if len(sel_compare) != 2:
            st.warning("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–æ–≤–Ω–æ 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.")
        else:
            idxs = [doc_names.index(n) for n in sel_compare]
            info1 = extract_money_percent_dates(st.session_state.docs[idxs[0]]['text'])
            info2 = extract_money_percent_dates(st.session_state.docs[idxs[1]]['text'])
            st.markdown(f"### {sel_compare[0]} vs {sel_compare[1]}")
            st.write("–ü—Ä–æ—Ü–µ–Ω—Ç—ã:", {sel_compare[0]: info1.get('percents',[]), sel_compare[1]: info2.get('percents',[])})
            st.write("–°—É–º–º—ã:", {sel_compare[0]: info1.get('sums',[]), sel_compare[1]: info2.get('sums',[])})
            st.write("–î–∞—Ç—ã:", {sel_compare[0]: info1.get('dates',[]), sel_compare[1]: info2.get('dates',[])})

# –í–æ–ø—Ä–æ—Å/–æ—Ç–≤–µ—Ç
st.subheader("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º")
question = st.text_input("–í–æ–ø—Ä–æ—Å:")
if st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å"):
    top_chunks = retrieve_top_k(question, st.session_state.chunks, st.session_state.embeddings)
    context = "\n\n".join(top_chunks)[:CONTEXT_LIMIT]
    answer = generate_answer(question, context)
    st.session_state.history.append({"q": question, "a": answer, "ctx": top_chunks})

# –ò—Å—Ç–æ—Ä–∏—è
st.markdown("### üí¨ –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤")
for item in reversed(st.session_state.history):
    st.markdown(f"**Q:** {item['q']}")
    st.markdown(f"**A:** {item['a']}")
    with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏"):
        for ch in item['ctx']:
            st.markdown(highlight_terms(ch, item['q']), unsafe_allow_html=True)

# –†–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
st.markdown("---")
st.subheader("üìå –ë—ã—Å—Ç—Ä–∞—è —Å–≤–æ–¥–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
doc_select = st.selectbox("–í—ã–±—Ä–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —Ä–µ–∑—é–º–µ", options=[d['name'] for d in st.session_state.docs])
if st.button("–°–¥–µ–ª–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ"):
    idx = [d['name'] for d in st.session_state.docs].index(doc_select)
    text_for_sum = " ".join(st.session_state.chunks[idx][:6])
    summ = generate_answer("–ö–æ—Ä–æ—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∂–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–∞—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π).", text_for_sum)
    st.success("–†–µ–∑—é–º–µ:")
    st.write(summ)
